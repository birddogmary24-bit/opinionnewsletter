import sys
import os
import datetime

# Add project root to path to allow imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from crawler.sources.youtube import YouTubeCrawler
from utils.db import get_db

from google.cloud import translate_v2 as translate
import re

def translate_text(text, target_language='ko'):
    """Translates text to target language if it contains significant non-Korean characters."""
    # Simple check: if it has no Hangul, try translating
    if not re.search('[ê°€-íž£]', text):
        try:
            translate_client = translate.Client()
            result = translate_client.translate(text, target_language=target_language)
            return result['translatedText']
        except Exception as e:
            print(f"Translation failed for '{text}': {e}")
            return text
    return text

def run_crawlers():
    print("ðŸš€ Starting Daily Crawler Job...")
    
    # 1. Define Sources
    sources = [
        {
            "type": "youtube",
            "name": "ìŠˆì¹´ì›”ë“œ",
            "url": "https://www.youtube.com/@syukaworld"
        },
        {
            "type": "youtube",
            "name": "ë§¤ê²½ ì›”ê°€ì›”ë¶€",
            "url": "https://www.youtube.com/@MK_Invest"
        }
    ]
    
    # 2. Initialize Crawlers
    yt_crawler = YouTubeCrawler()
    db = get_db()
    
    all_content = []
    
    # 3. Process Sources
    for source in sources:
        if source['type'] == 'youtube':
            print(f"Crawling {source['name']} ({source['url']})...")
            videos = yt_crawler.fetch_latest_videos(
                source['url'], 
                limit=10, 
                opinion_leader_name=source['name']
            )
            all_content.extend(videos)
            
    # 4. Save to Database
    print(f"ðŸ’¾ Saving {len(all_content)} items to Firestore...")
    collection_ref = db.collection('contents')
    
    saved_count = 0
    for item in all_content:
        # Translate Title if needed
        original_title = item['title']
        translated_title = translate_text(original_title)
        if original_title != translated_title:
             print(f"   - Translated: '{original_title}' -> '{translated_title}'")
             item['title'] = translated_title

        # Create a unique ID based on source and original ID to prevent duplicates
        doc_id = f"{item['source_type']}_{item['original_id']}"
        
        # Check if exists first to preserve discovery date (scraped_at)
        doc_ref = collection_ref.document(doc_id)
        doc_snapshot = doc_ref.get()
        
        if doc_snapshot.exists:
            # Update only dynamic fields, preserve scraped_at
            print(f"   - Updating views for: {item['title']}")
            doc_ref.update({
                'view_count': item.get('view_count'),
                'description': item.get('description') # Update description if it was improved
            })
        else:
            # New Discovery
            item['scraped_at'] = datetime.datetime.now()
            doc_ref.set(item)
            print(f"   - Newly Discovered: {item['title']}")
            saved_count += 1
        
    print(f"âœ… Job Complete. {saved_count} items processed.")

if __name__ == "__main__":
    run_crawlers()
