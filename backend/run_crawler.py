import sys
import os
import datetime

# Add project root to path to allow imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from crawler.sources.youtube import YouTubeCrawler
from utils.db import get_db

from google.cloud import translate_v2 as translate
import re

def translate_text(text, target_language='ko'):
    """Translates text to target language if it contains significant non-Korean characters."""
    # Simple check: if it has no Hangul, try translating
    if not re.search('[ê°€-íž£]', text):
        try:
            translate_client = translate.Client()
            result = translate_client.translate(text, target_language=target_language)
            return result['translatedText']
        except Exception as e:
            print(f"Translation failed for '{text}': {e}")
            return text
    return text

def run_crawlers():
    print("ðŸš€ Starting Daily Crawler Job...")
    
    # 1. Define Sources (Comprehensive list of Opinion Leaders)
    sources = [
        # ê²½ì œ/íˆ¬ìž (Economy & Finance)
        {"type": "youtube", "name": "ìŠˆì¹´ì›”ë“œ", "url": "https://www.youtube.com/channel/UCsJ6RuBiTVWRX156FVbeaGg", "category": "ê²½ì œ"},
        {"type": "youtube", "name": "ë§¤ê²½ ì›”ê°€ì›”ë¶€", "url": "https://www.youtube.com/channel/UCIipmgxpUxDmPP-ma3Ahvbw", "category": "ê²½ì œ"},
        {"type": "youtube", "name": "í™ì¶˜ìš±ì˜ ê²½ì œê°•ì˜ë…¸íŠ¸", "url": "https://www.youtube.com/channel/UCmNbuxmvRVv9OcdAO0cpLnw", "category": "ê²½ì œ"},
        
        # ë¶€ë™ì‚° (Real Estate)
        {"type": "youtube", "name": "ì›”ê¸‰ìŸì´ë¶€ìžë“¤TV", "url": "https://www.youtube.com/channel/UCDSj40X9FFUAnx1nv7gQhcA", "category": "ë¶€ë™ì‚°"},
        {"type": "youtube", "name": "ë¶€ì½ë‚¨TV", "url": "https://www.youtube.com/channel/UC2QeHNJFfuQWB4cy3M-745g", "category": "ë¶€ë™ì‚°"},
        {"type": "youtube", "name": "ìž¬í…Œí¬ ì½ì–´ì£¼ëŠ” íŒŒì¼ëŸ¿", "url": "https://www.youtube.com/@pilot_money", "category": "ë¶€ë™ì‚°"},
        
        # IT/ì½”ë”© (IT & Tech)
        {"type": "youtube", "name": "ì¡°ì½”ë”©", "url": "https://www.youtube.com/@jocoding", "category": "IT"},
        {"type": "youtube", "name": "ë…¸ë§ˆë“œ ì½”ë”", "url": "https://www.youtube.com/@nomadcoders", "category": "IT"},
        {"type": "youtube", "name": "í¬í”„TV", "url": "https://www.youtube.com/@popekim", "category": "IT"},
        
        # ê³¼í•™ (Science)
        {"type": "youtube", "name": "ì•ˆë ê³¼í•™", "url": "https://www.youtube.com/@the_AS", "category": "ê³¼í•™"},
        {"type": "youtube", "name": "ê¶¤ë„", "url": "https://www.youtube.com/@science_orbit", "category": "ê³¼í•™"},
        {"type": "youtube", "name": "ê³¼í•™ë“œë¦¼", "url": "https://www.youtube.com/@ScienceDream", "category": "ê³¼í•™"}
    ]
    
    # 2. Initialize Crawlers
    yt_crawler = YouTubeCrawler()
    db = get_db()
    
    all_content = []
    
    # 3. Process Sources
    for source in sources:
        if source['type'] == 'youtube':
            print(f"Crawling {source['name']} ({source['url']})...")
            videos = yt_crawler.fetch_latest_videos(
                source['url'], 
                limit=10, 
                opinion_leader_name=source['name']
            )
            # Add category to each video item
            for v in videos:
                v['category'] = source.get('category', 'ê¸°íƒ€')
            all_content.extend(videos)
            
    # 4. Save to Database
    print(f"ðŸ’¾ Saving {len(all_content)} items to Firestore...")
    collection_ref = db.collection('contents')
    
    saved_count = 0
    for item in all_content:
        # Translate Title if needed
        original_title = item['title']
        translated_title = translate_text(original_title)
        if original_title != translated_title:
             print(f"   - Translated: '{original_title}' -> '{translated_title}'")
             item['title'] = translated_title

        # Create a unique ID based on source and original ID to prevent duplicates
        doc_id = f"{item['source_type']}_{item['original_id']}"
        
        # Check if exists first to preserve discovery date (scraped_at)
        doc_ref = collection_ref.document(doc_id)
        doc_snapshot = doc_ref.get()
        
        if doc_snapshot.exists:
            # Update only dynamic fields, preserve scraped_at
            print(f"   - Updating views for: {item['title']}")
            doc_ref.update({
                'view_count': item.get('view_count'),
                'description': item.get('description') # Update description if it was improved
            })
        else:
            # New Discovery
            item['scraped_at'] = datetime.datetime.now()
            doc_ref.set(item)
            print(f"   - Newly Discovered: {item['title']}")
            saved_count += 1
        
    print(f"âœ… Job Complete. {saved_count} items processed.")

if __name__ == "__main__":
    run_crawlers()
